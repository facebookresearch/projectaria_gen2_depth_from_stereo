{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f55d3b",
   "metadata": {},
   "source": [
    "# Tutorial: Depth-from-Stereo Estimation with Foundation Stereo\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to compute metric depth maps from Aria Gen2 stereo cameras using stereo rectification and the Foundation Stereo neural network. You will learn the complete pipeline from loading VRS data to visualizing 3D depth in Rerun.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Load stereo camera data from Aria Gen2 VRS files\n",
    "- Perform stereo rectification on fisheye images to align epipolar lines\n",
    "- Use Foundation Stereo for zero-shot disparity estimation\n",
    "- Convert disparity to metric depth using camera calibration\n",
    "- Visualize depth as interactive 3D point clouds with Rerun\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Stereo Rectification**: Process of transforming stereo images so epipolar lines are horizontal, simplifying stereo matching from 2D to 1D search\n",
    "- **Disparity**: Horizontal pixel displacement between corresponding points in left/right images\n",
    "- **Depth from Disparity**: `Z = (baseline × focal_length) / disparity`\n",
    "- **Foundation Stereo**: Zero-shot stereo matching neural network that works across diverse scenes without fine-tuning\n",
    "\n",
    "**Prerequisites:**\n",
    "- CUDA-capable GPU (2-4GB VRAM)\n",
    "- Familiarity with Project Aria Tools (see Tutorial 1: VrsDataProvider Basics)\n",
    "- Understanding of camera calibration concepts (see Tutorial 2: Device Calibration)\n",
    "- Basic understanding of stereo geometry\n",
    "\n",
    "### ⚠️ Important Notes\n",
    "- **Google Colab Users:**  \n",
    "  If you encounter a `ModuleNotFoundError: No module named 'rerun'` error after installing `rerun-sdk`, Colab may not recognize the new package until the runtime is restarted.  \n",
    "  **Fix:** Go to **Runtime → Restart session and run all**.\n",
    "\n",
    "- **Visualization Issue:**  \n",
    "  If a Rerun visualization window does not appear, this may be due to a known caching issue. Simply re-run the visualization cell to resolve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jjvlsf6jm9",
   "metadata": {},
   "source": [
    "## Setup Environment (Google Colab)\n",
    "\n",
    "If running on Google Colab, install `projectaria-tools`, Foundation Stereo dependencies, and download sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saztdg3nfjj",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\n\ngoogle_colab_env = 'google.colab' in str(get_ipython())\n\nif google_colab_env:\n    print(\"Running from Google Colab, installing dependencies and downloading sample data\")\n\n    # Install Project Aria Tools and tutorial dependencies\n    !pip install projectaria-tools['all']==2.1.0\n    !pip install omegaconf rerun-sdk[notebook]\n\n    # Install Foundation Stereo dependencies (some may already be in Colab)\n    !pip install trimesh scikit-image opencv-contrib-python imgaug timm einops \\\n        open3d transformations xformers==0.0.28.post1 huggingface-hub \\\n        albumentations ruamel.yaml\n\n    # flash-attn is optional: Foundation Stereo uses a fallback if unavailable.\n    # It requires CUDA toolkit to compile and may fail on some Colab runtimes.\n    !pip install flash-attn --no-build-isolation 2>/dev/null \\\n        || echo \"flash-attn install failed (optional, will use fallback attention)\"\n\n    # Download stereo_utils.py from the tutorial repo\n    if not os.path.exists('./stereo_utils.py'):\n        !curl -sL -o stereo_utils.py \\\n            \"https://raw.githubusercontent.com/facebookresearch/projectaria_gen2_depth_from_stereo/main/stereo_utils.py\"\n        print(\"Downloaded stereo_utils.py\")\n\n    # Clone Foundation Stereo\n    if not os.path.exists('./FoundationStereo'):\n        !git clone https://github.com/NVlabs/FoundationStereo.git\n\n    # Download pretrained model weights from Hugging Face Hub.\n    # Default: 23-51-11 (ViT-Large, best accuracy).\n    # Alternative: 11-33-40 (ViT-Small, faster).\n    # See https://github.com/NVlabs/FoundationStereo for all available models.\n    from huggingface_hub import hf_hub_download\n\n    ckpt_dir = \"./FoundationStereo/pretrained_models/23-51-11\"\n    os.makedirs(ckpt_dir, exist_ok=True)\n    ckpt_path = os.path.join(ckpt_dir, \"model_best_bp2.pth\")\n    cfg_path = os.path.join(ckpt_dir, \"cfg.yaml\")\n\n    HF_REPO = \"shriarul5273/FoundationStereo_models\"\n\n    if not os.path.exists(cfg_path):\n        print(\"Downloading model config...\")\n        hf_hub_download(\n            repo_id=HF_REPO,\n            filename=\"pretrained_models/23-51-11/cfg.yaml\",\n            local_dir=\"./FoundationStereo\",\n        )\n        print(f\"  Config saved to {cfg_path}\")\n\n    if not os.path.exists(ckpt_path):\n        print(\"Downloading model checkpoint (~3.3GB)...\")\n        hf_hub_download(\n            repo_id=HF_REPO,\n            filename=\"pretrained_models/23-51-11/model_best_bp2.pth\",\n            local_dir=\"./FoundationStereo\",\n        )\n        print(f\"  Checkpoint saved to {ckpt_path}\")\n\n    # Verify checkpoint is not a corrupt/partial download\n    ckpt_size = os.path.getsize(ckpt_path)\n    if ckpt_size < 1_000_000:\n        os.remove(ckpt_path)\n        raise RuntimeError(\n            f\"Downloaded checkpoint is too small ({ckpt_size} bytes), likely corrupt.\\n\"\n            \"Please re-run this cell to retry the download.\"\n        )\n\n    print(f\"Model weights ready at {ckpt_dir}\")\n    foundation_stereo_ckpt = ckpt_path\n\n    # Download sample VRS\n    vrs_sample_path = \"./vrs_sample_data\"\n    vrs_url = \"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs\"\n    vrs_filename = \"aria_gen2_sample_data_1.vrs\"\n    vrs_file_path = os.path.join(vrs_sample_path, vrs_filename)\n\n    if not os.path.exists(vrs_file_path):\n        command_list = [\n            f\"mkdir -p {vrs_sample_path}\",\n            f'curl -o {vrs_file_path} -C - -O -L \"{vrs_url}\"'\n        ]\n        print(f\"Downloading VRS sample data to {vrs_file_path}...\")\n        for command in command_list:\n            !$command\n        print(f\"Download complete! VRS file saved to: {vrs_file_path}\")\n    else:\n        print(f\"VRS file already exists at: {vrs_file_path}\")\n\n    # Trigger early import of rerun to surface install issues.\n    # If this fails with ModuleNotFoundError, go to Runtime -> Restart session\n    # and run all cells again.\n    import rerun as rr\nelse:\n    # Local environment - update these paths\n    vrs_file_path = \"path/to/your/recording.vrs\"\n    foundation_stereo_ckpt = \"./FoundationStereo/pretrained_models/23-51-11/model_best_bp2.pth\"\n    print(\"Please update vrs_file_path to point to an Aria-Gen2 VRS file on your system\")"
  },
  {
   "cell_type": "markdown",
   "id": "df6706a6",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll import all required libraries and verify GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical and visualization imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch for Foundation Stereo\n",
    "import torch\n",
    "\n",
    "# Project Aria Tools imports\n",
    "from projectaria_tools.core import data_provider, calibration\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "from projectaria_tools.core.sophus import SE3, SO3\n",
    "from projectaria_tools.core.image import InterpolationMethod\n",
    "\n",
    "# Rerun for 3D visualization\n",
    "import rerun as rr\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49a98b",
   "metadata": {},
   "outputs": [],
   "source": "# Foundation Stereo Setup\nfrom pathlib import Path\n\n# For Google Colab: clone repository\nif 'google.colab' in str(get_ipython()):\n    if not Path('./FoundationStereo').exists():\n        !git clone https://github.com/NVlabs/FoundationStereo.git\n    FOUNDATION_STEREO_PATH = './FoundationStereo'\nelse:\n    # Local environment: check common locations or use environment variable\n    search_paths = [\n        os.environ.get('FOUNDATION_STEREO_PATH'),\n        Path.cwd() / 'FoundationStereo',\n        Path.home() / 'FoundationStereo',\n        Path.home() / 'projects' / 'FoundationStereo',\n        Path.home() / 'projects' / 'nebula_foundation_stereo_notebook' / 'FoundationStereo',\n    ]\n\n    FOUNDATION_STEREO_PATH = None\n    for path in search_paths:\n        if path and Path(path).exists() and (Path(path) / 'core').exists():\n            FOUNDATION_STEREO_PATH = str(path)\n            break\n\n    if FOUNDATION_STEREO_PATH is None:\n        raise FileNotFoundError(\n            \"FoundationStereo not found. Please either:\\n\"\n            \"  1. git clone https://github.com/NVlabs/FoundationStereo.git\\n\"\n            \"  2. Set FOUNDATION_STEREO_PATH environment variable\\n\"\n        )\n\n# Add to Python path\nsys.path.insert(0, FOUNDATION_STEREO_PATH)\n\n# Import Foundation Stereo components\nfrom omegaconf import OmegaConf\nfrom core.foundation_stereo import FoundationStereo\nfrom core.utils.utils import InputPadder\n\n# Stereo utilities (absolute import required -- relative imports are not supported in notebooks)\nfrom stereo_utils import (\n    create_scanline_rectified_cameras,\n    fisheye_to_linear_calib,\n    rectify_stereo_pair,\n    compute_stereo_baseline,\n    disparity_to_depth,\n    get_rectified_camera_transform,\n)\n\nprint(f\"Foundation Stereo loaded from: {FOUNDATION_STEREO_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(\"System Information:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"\\nGPU ready for Foundation Stereo inference\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No GPU detected. Foundation Stereo requires CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfc167",
   "metadata": {},
   "source": [
    "## 2. Configure Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b13be1",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\n# VRS file path - update for your local environment\nVRS_FILE_PATH = vrs_file_path if 'vrs_file_path' in dir() else \"path/to/your/recording.vrs\"\n\n# Foundation Stereo checkpoint\n# Use the path set in the Colab setup cell, or build from FOUNDATION_STEREO_PATH\nif 'foundation_stereo_ckpt' in dir():\n    FOUNDATION_STEREO_CKPT = foundation_stereo_ckpt\nelse:\n    FOUNDATION_STEREO_CKPT = os.path.join(FOUNDATION_STEREO_PATH, \"pretrained_models/23-51-11/model_best_bp2.pth\")\n\n# Frame to process\nFRAME_INDEX = 100\n\n# Rectification parameters\nOUTPUT_WIDTH = 512\nOUTPUT_HEIGHT = 512\nFOCAL_SCALE = 1.25\n\n# Foundation Stereo parameters\nVALID_ITERS = 32\n\n# Depth parameters\nMIN_DISPARITY = 1.0\nMAX_DEPTH = 20.0\n\nprint(f\"VRS file: {VRS_FILE_PATH}\")\nprint(f\"Checkpoint: {FOUNDATION_STEREO_CKPT}\")\nprint(f\"Output: {OUTPUT_WIDTH}x{OUTPUT_HEIGHT}\")"
  },
  {
   "cell_type": "markdown",
   "id": "b780877c",
   "metadata": {},
   "source": [
    "## 3. Load VRS Data and Extract Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ddc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VRS file\n",
    "print(f\"Loading VRS file: {VRS_FILE_PATH}\")\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(VRS_FILE_PATH)\n",
    "assert vrs_data_provider is not None, f\"Failed to load VRS file\"\n",
    "\n",
    "device_calib = vrs_data_provider.get_device_calibration()\n",
    "\n",
    "# Identify stereo camera streams\n",
    "left_stream_id = vrs_data_provider.get_stream_id_from_label(\"slam-front-left\")\n",
    "right_stream_id = vrs_data_provider.get_stream_id_from_label(\"slam-front-right\")\n",
    "\n",
    "assert left_stream_id is not None, \"Could not find slam-front-left stream\"\n",
    "assert right_stream_id is not None, \"Could not find slam-front-right stream\"\n",
    "\n",
    "left_num_frames = vrs_data_provider.get_num_data(left_stream_id)\n",
    "right_num_frames = vrs_data_provider.get_num_data(right_stream_id)\n",
    "\n",
    "print(f\"VRS file loaded\")\n",
    "print(f\"  Left: {left_stream_id}, {left_num_frames} frames\")\n",
    "print(f\"  Right: {right_stream_id}, {right_num_frames} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get camera calibrations\n",
    "left_calib = device_calib.get_camera_calib(\"slam-front-left\")\n",
    "right_calib = device_calib.get_camera_calib(\"slam-front-right\")\n",
    "\n",
    "T_left_cam_device = left_calib.get_transform_device_camera().inverse()\n",
    "T_right_cam_device = right_calib.get_transform_device_camera().inverse()\n",
    "\n",
    "baseline = compute_stereo_baseline(T_left_cam_device, T_right_cam_device)\n",
    "\n",
    "print(\"Camera Calibration:\")\n",
    "print(f\"  Left: {left_calib.get_model_name()}, {left_calib.get_image_size()}\")\n",
    "print(f\"  Right: {right_calib.get_model_name()}, {right_calib.get_image_size()}\")\n",
    "print(f\"  Baseline: {baseline*1000:.1f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe1d48",
   "metadata": {},
   "source": [
    "## 4. Load Stereo Frame Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "left_data, left_record = vrs_data_provider.get_image_data_by_index(left_stream_id, FRAME_INDEX)\n",
    "right_data, right_record = vrs_data_provider.get_image_data_by_index(right_stream_id, FRAME_INDEX)\n",
    "\n",
    "left_image = left_data.to_numpy_array()\n",
    "right_image = right_data.to_numpy_array()\n",
    "timestamp_ns = left_record.capture_timestamp_ns\n",
    "\n",
    "print(f\"Frame {FRAME_INDEX} loaded\")\n",
    "print(f\"  Shape: {left_image.shape}\")\n",
    "print(f\"  Timestamp: {timestamp_ns} ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original fisheye images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].imshow(left_image, cmap='gray')\n",
    "axes[0].set_title(f'Original Left (Fisheye) - Frame {FRAME_INDEX}')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(right_image, cmap='gray')\n",
    "axes[1].set_title(f'Original Right (Fisheye) - Frame {FRAME_INDEX}')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b61a16",
   "metadata": {},
   "source": [
    "## 5. Stereo Rectification\n",
    "\n",
    "**Understanding Stereo Rectification:**\n",
    "\n",
    "Stereo rectification transforms the stereo pair to satisfy:\n",
    "1. **Horizontal epipolar lines** - corresponding points lie on the same row\n",
    "2. **No lens distortion** - fisheye distortion is removed  \n",
    "3. **Common image plane** - both cameras project onto aligned planes\n",
    "\n",
    "This simplifies stereo matching from 2D to 1D search (along each row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear (pinhole) camera calibrations\n",
    "print(\"Creating rectified camera models...\")\n",
    "\n",
    "left_linear = fisheye_to_linear_calib(\n",
    "    left_calib, focal_scale=FOCAL_SCALE,\n",
    "    output_width=OUTPUT_WIDTH, output_height=OUTPUT_HEIGHT\n",
    ")\n",
    "right_linear = fisheye_to_linear_calib(\n",
    "    right_calib, focal_scale=FOCAL_SCALE,\n",
    "    output_width=OUTPUT_WIDTH, output_height=OUTPUT_HEIGHT\n",
    ")\n",
    "\n",
    "print(f\"Linear models created: {OUTPUT_WIDTH}x{OUTPUT_HEIGHT}\")\n",
    "print(f\"  Focal length: {left_linear.get_projection_params()[0]:.1f} px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rectification rotations\n",
    "Rl_n, Rr_n = create_scanline_rectified_cameras(T_left_cam_device, T_right_cam_device)\n",
    "print(\"Rectification rotations computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd884d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rectification\n",
    "start_time = time.time()\n",
    "left_rectified, right_rectified = rectify_stereo_pair(\n",
    "    left_image, right_image, left_calib, right_calib,\n",
    "    left_linear, right_linear, Rl_n, Rr_n,\n",
    "    interpolation=InterpolationMethod.BILINEAR\n",
    ")\n",
    "rectify_time = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"Rectification complete in {rectify_time:.1f} ms\")\n",
    "print(f\"  Output shape: {left_rectified.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72292018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rectified images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].imshow(left_rectified, cmap='gray')\n",
    "axes[0].set_title('Rectified Left (Pinhole)')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(right_rectified, cmap='gray')\n",
    "axes[1].set_title('Rectified Right (Pinhole)')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd79154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify epipolar alignment\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "stacked = np.hstack([left_rectified, right_rectified])\n",
    "ax.imshow(stacked, cmap='gray')\n",
    "for y in range(0, OUTPUT_HEIGHT, 40):\n",
    "    ax.axhline(y, color='red', linewidth=1.0, alpha=1.0)\n",
    "ax.set_title('Epipolar Line Alignment\\n(Red lines show corresponding rows)')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Rectification verified: epipolar lines are horizontal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cf543",
   "metadata": {},
   "source": [
    "## 6. Foundation Stereo Inference\n",
    "\n",
    "Foundation Stereo is a zero-shot stereo matching neural network that computes disparity without requiring fine-tuning for new scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5b4be",
   "metadata": {},
   "outputs": [],
   "source": "# Load Foundation Stereo model\nprint(f\"Loading Foundation Stereo...\")\ncfg_path = os.path.join(os.path.dirname(FOUNDATION_STEREO_CKPT), 'cfg.yaml')\ncfg = OmegaConf.load(cfg_path)\ncfg.valid_iters = VALID_ITERS\n\n# Auto-detect ViT size from checkpoint weights to avoid architecture mismatch.\n# The model config may not specify vit_size, or the user may have downloaded\n# a different checkpoint variant than what cfg.yaml expects.\nckpt = torch.load(FOUNDATION_STEREO_CKPT, map_location='cpu', weights_only=False)\nembed_key = 'feature.dino.depth_anything.pretrained.cls_token'\nif embed_key in ckpt['model']:\n    embed_dim = ckpt['model'][embed_key].shape[-1]\n    vit_size_map = {384: 'vits', 768: 'vitb', 1024: 'vitl'}\n    if embed_dim in vit_size_map:\n        cfg['vit_size'] = vit_size_map[embed_dim]\n        print(f\"  Detected {cfg['vit_size']} checkpoint (embed_dim={embed_dim})\")\n    else:\n        cfg['vit_size'] = 'vitl'\n        print(f\"  Unknown embed_dim={embed_dim}, defaulting to vitl\")\nelif 'vit_size' not in cfg:\n    cfg['vit_size'] = 'vitl'\n\nmodel = FoundationStereo(cfg)\nmodel.load_state_dict(ckpt['model'])\ndel ckpt  # Free CPU memory (~1.8GB)\nmodel = model.cuda().eval()\n\nparam_count = sum(p.numel() for p in model.parameters()) / 1e6\nprint(f\"Model loaded: ViT-{cfg.vit_size} ({param_count:.0f}M params), {cfg.valid_iters} iterations\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input tensors\n",
    "if len(left_rectified.shape) == 2:\n",
    "    left_rgb = np.stack([left_rectified] * 3, axis=-1)\n",
    "    right_rgb = np.stack([right_rectified] * 3, axis=-1)\n",
    "else:\n",
    "    left_rgb = left_rectified\n",
    "    right_rgb = right_rectified\n",
    "\n",
    "left_tensor = torch.from_numpy(left_rgb).float().cuda().permute(2, 0, 1).unsqueeze(0)\n",
    "right_tensor = torch.from_numpy(right_rgb).float().cuda().permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "padder = InputPadder(left_tensor.shape, divis_by=32, force_square=False)\n",
    "left_padded, right_padded = padder.pad(left_tensor, right_tensor)\n",
    "\n",
    "print(f\"Input shape: {left_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "print(\"Running Foundation Stereo...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.cuda.amp.autocast(True):\n",
    "    with torch.no_grad():\n",
    "        disparity = model.forward(left_padded, right_padded, iters=cfg.valid_iters, test_mode=True)\n",
    "\n",
    "inference_time = (time.time() - start_time) * 1000\n",
    "disparity = padder.unpad(disparity.float())\n",
    "disparity_map = disparity.cpu().numpy().squeeze()\n",
    "\n",
    "print(f\"Inference complete in {inference_time:.1f} ms\")\n",
    "print(f\"  Disparity range: [{disparity_map.min():.2f}, {disparity_map.max():.2f}] px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disparity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].imshow(left_rectified, cmap='gray')\n",
    "axes[0].set_title('Left Rectified')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(right_rectified, cmap='gray')\n",
    "axes[1].set_title('Right Rectified')\n",
    "axes[1].axis('off')\n",
    "disp_vis = axes[2].imshow(disparity_map, cmap='magma')\n",
    "axes[2].set_title('Disparity Map')\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(disp_vis, ax=axes[2], fraction=0.046)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063e8bb",
   "metadata": {},
   "source": [
    "## 7. Convert Disparity to Depth\n",
    "\n",
    "We convert disparity to metric depth using: **`depth = (baseline × focal_length) / disparity`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get focal length\n",
    "focal_length = left_linear.get_projection_params()[0]\n",
    "print(f\"Baseline: {baseline:.4f} m ({baseline*1000:.1f} mm)\")\n",
    "print(f\"Focal length: {focal_length:.2f} px\")\n",
    "print(f\"Formula: depth = ({baseline:.4f} × {focal_length:.2f}) / disparity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to depth\n",
    "depth_map = disparity_to_depth(\n",
    "    disparity_map, baseline=baseline, focal_length=focal_length,\n",
    "    min_disparity=MIN_DISPARITY, max_depth=MAX_DEPTH\n",
    ")\n",
    "\n",
    "valid_depth = depth_map[depth_map > 0]\n",
    "print(f\"\\nDepth map computed\")\n",
    "print(f\"  Valid pixels: {len(valid_depth)/depth_map.size*100:.1f}%\")\n",
    "print(f\"  Depth range: [{valid_depth.min():.2f}, {valid_depth.max():.2f}] m\")\n",
    "print(f\"  Mean depth: {valid_depth.mean():.2f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize depth\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes[0,0].imshow(left_rectified, cmap='gray')\n",
    "axes[0,0].set_title('Left Rectified')\n",
    "axes[0,0].axis('off')\n",
    "disp_vis = axes[0,1].imshow(disparity_map, cmap='magma')\n",
    "axes[0,1].set_title('Disparity (px)')\n",
    "axes[0,1].axis('off')\n",
    "plt.colorbar(disp_vis, ax=axes[0,1], fraction=0.046)\n",
    "depth_vis = axes[1,0].imshow(depth_map, cmap='turbo', vmin=0, vmax=10)\n",
    "axes[1,0].set_title('Depth (meters)')\n",
    "axes[1,0].axis('off')\n",
    "plt.colorbar(depth_vis, ax=axes[1,0], fraction=0.046)\n",
    "axes[1,1].hist(valid_depth, bins=50, color='steelblue', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Depth (m)')\n",
    "axes[1,1].set_ylabel('Pixel Count')\n",
    "axes[1,1].set_title('Depth Distribution')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6014b",
   "metadata": {},
   "source": [
    "## 8. 3D Visualization with Rerun\n",
    "\n",
    "Visualize the depth map as a 3D point cloud using Rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Rerun\n",
    "rr.init(\"aria_stereo_depth_tutorial\")\n",
    "rr.set_time_nanos(\"device_time\", timestamp_ns)\n",
    "print(\"Rerun initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rectified camera transform\n",
    "# T_rect_device transforms: device -> rectified camera\n",
    "# For Rerun, we need camera-to-world (camera-to-device), so invert it\n",
    "T_rect_device = get_rectified_camera_transform(T_left_cam_device, Rl_n)\n",
    "T_device_rect = T_rect_device.inverse()  # camera -> device (for Rerun)\n",
    "\n",
    "fx, fy = left_linear.get_projection_params()[0], left_linear.get_projection_params()[1]\n",
    "cx, cy = left_linear.get_projection_params()[2], left_linear.get_projection_params()[3]\n",
    "\n",
    "# Log camera\n",
    "rr.log(\n",
    "    \"world/camera\",\n",
    "    rr.Pinhole(\n",
    "        resolution=[OUTPUT_WIDTH, OUTPUT_HEIGHT],\n",
    "        focal_length=[fx, fy],\n",
    "        principal_point=[cx, cy],\n",
    "    ),\n",
    "    static=True\n",
    ")\n",
    "rr.log(\n",
    "    \"world/camera\",\n",
    "    rr.Transform3D(\n",
    "        translation=T_device_rect.translation(),\n",
    "        mat3x3=T_device_rect.rotation().to_matrix(),\n",
    "    )\n",
    ")\n",
    "print(\"Camera logged to Rerun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log stereo images and disparity to Rerun\n",
    "\n",
    "# Log left rectified image\n",
    "rr.log(\"images/left_rectified\", rr.Image(left_rectified))\n",
    "\n",
    "# Log right rectified image\n",
    "rr.log(\"images/right_rectified\", rr.Image(right_rectified))\n",
    "\n",
    "# Create colored disparity visualization using matplotlib colormap\n",
    "import matplotlib.cm as cm\n",
    "disp_normalized = (disparity_map - disparity_map.min()) / (disparity_map.max() - disparity_map.min() + 1e-6)\n",
    "disp_colored = (cm.magma(disp_normalized)[:, :, :3] * 255).astype(np.uint8)\n",
    "rr.log(\"images/disparity\", rr.Image(disp_colored))\n",
    "\n",
    "# Log depth as 3D point cloud with the left image as texture\n",
    "depth_mm = (depth_map * 1000).astype(np.uint16)\n",
    "rr.log(\n",
    "    \"world/camera\",\n",
    "    rr.DepthImage(\n",
    "        depth_mm,\n",
    "        meter=1000.0,\n",
    "        colormap=\"Turbo\",\n",
    "        point_fill_ratio=0.5,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Images logged to Rerun:\")\n",
    "print(\"  - images/left_rectified: Left rectified grayscale\")\n",
    "print(\"  - images/right_rectified: Right rectified grayscale\")\n",
    "print(\"  - images/disparity: Colored disparity map (magma)\")\n",
    "print(\"  - world/camera: 3D point cloud with depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d746096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Rerun visualization with custom layout\n",
    "import rerun.blueprint as rrb\n",
    "\n",
    "# Create blueprint with large 3D view on top, images in a row at bottom\n",
    "blueprint = rrb.Vertical(\n",
    "    # Main 3D view (takes most of the space)\n",
    "    rrb.Spatial3DView(name=\"3D Depth\", origin=\"world\"),\n",
    "    # Row of image views at the bottom\n",
    "    rrb.Horizontal(\n",
    "        rrb.Spatial2DView(name=\"Left Rectified\", origin=\"images/left_rectified\"),\n",
    "        rrb.Spatial2DView(name=\"Right Rectified\", origin=\"images/right_rectified\"),\n",
    "        rrb.Spatial2DView(name=\"Disparity\", origin=\"images/disparity\"),\n",
    "    ),\n",
    "    row_shares=[3, 1],  # 3:1 ratio - 3D view is 3x taller than image row\n",
    ")\n",
    "\n",
    "# Send blueprint and display in notebook\n",
    "rr.send_blueprint(blueprint)\n",
    "rr.notebook_show()\n",
    "\n",
    "print(\"\\n3D visualization ready!\")\n",
    "print(\"\\nLayout:\")\n",
    "print(\"  - Top (large): 3D point cloud view\")\n",
    "print(\"  - Bottom row: Left | Right | Disparity\")"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "fe13a016-5e18-4ba6-adca-4ab810b81609",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "foundation_stereo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}